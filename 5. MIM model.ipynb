{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bead4bd2",
   "metadata": {},
   "source": [
    "# FaX AI with SHAP and AIF360 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677ae54",
   "metadata": {},
   "source": [
    "This example presents how we can use the FaX AI, [AIF360](https://github.com/Trusted-AI/AIF360), and [SHAP](https://github.com/slundberg/shap) libraries together for marrying fairness with explainability.  \n",
    "\n",
    "We use the COMPAS dataset (using the AIF360 loading methods) and how to train and measure our methods, the methods implemented in the AIF360 library, and sklearn supervised learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78556d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_eager_execution()\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from aif360.sklearn.preprocessing import ReweighingMeta\n",
    "from aif360.sklearn.inprocessing import AdversarialDebiasing, ExponentiatedGradientReduction, GridSearchReduction\n",
    "from aif360.sklearn.postprocessing import CalibratedEqualizedOdds, PostProcessingMeta\n",
    "from aif360.sklearn.datasets import fetch_adult, fetch_compas, fetch_german\n",
    "from aif360.sklearn.metrics import disparate_impact_ratio, average_odds_error, generalized_fpr\n",
    "from aif360.sklearn.metrics import generalized_fnr, difference, statistical_parity_difference,equal_opportunity_difference\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "import FaX_methods\n",
    "import aif360_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2ff41",
   "metadata": {},
   "source": [
    "### Training MIM from FaX AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd00c363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>aa</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abc</th>\n",
       "      <th>aberration</th>\n",
       "      <th>...</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zionists</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zuma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13756 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  aa  ab  abandon  abandoned  abandoning  abandonment  abbott  \\\n",
       "0           0   0   0        0          0           0            0       0   \n",
       "1           1   0   0        0          0           0            0       0   \n",
       "2           2   0   0        0          0           0            0       0   \n",
       "3           3   0   0        0          0           0            0       0   \n",
       "4           4   0   0        0          0           0            0       0   \n",
       "\n",
       "   abc  aberration  ...  zimmerman  zionism  zionist  zionists  zip  zombie  \\\n",
       "0    0           0  ...          0        0        0         0    0       0   \n",
       "1    0           0  ...          0        0        0         0    0       0   \n",
       "2    0           0  ...          0        0        0         0    0       0   \n",
       "3    0           0  ...          0        0        0         0    0       0   \n",
       "4    0           0  ...          0        0        0         0    0       0   \n",
       "\n",
       "   zombies  zone  zones  zuma  \n",
       "0        0     0      0     0  \n",
       "1        0     0      0     0  \n",
       "2        0     0      0     0  \n",
       "3        0     0      0     0  \n",
       "4        0     0      0     0  \n",
       "\n",
       "[5 rows x 13756 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"X_vect.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c65486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66708"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6034f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6867d201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66708"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1deea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['prot_word_count'] = train_df['prot_word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f80488",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0263702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non toxic comments: 41566\n",
      "Number of toxic comments: 25142\n"
     ]
    }
   ],
   "source": [
    "# the number of non toxic comments was more than 10 times the number of toxic comments, we have subsampled it to the following distribution\n",
    "print(\"Number of non toxic comments: \"+str(len(data[data[\"target\"] < 0.5])))\n",
    "print(\"Number of toxic comments: \"+str(len(data[data[\"target\"] >= 0.5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5596b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(66000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5ca8177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non toxic comments: 41113\n",
      "Number of toxic comments: 24887\n"
     ]
    }
   ],
   "source": [
    "# the number of non toxic comments was more than 10 times the number of toxic comments, we have subsampled it to the following distribution\n",
    "print(\"Number of non toxic comments: \"+str(len(sample[sample[\"target\"] < 0.5])))\n",
    "print(\"Number of toxic comments: \"+str(len(sample[sample[\"target\"] >= 0.5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04f4d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample['target'].apply(lambda x: x > .5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26f9a3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12758    1\n",
       "30168    1\n",
       "9135     0\n",
       "17387    1\n",
       "59336    0\n",
       "        ..\n",
       "23898    1\n",
       "23884    1\n",
       "5812     1\n",
       "7947     0\n",
       "41514    1\n",
       "Name: target, Length: 66000, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "068af16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'aa', 'ab', 'abandon', 'abandoned', 'abandoning',\n",
       "       'abandonment', 'abbott', 'abc', 'aberration',\n",
       "       ...\n",
       "       'zionism', 'zionist', 'zionists', 'zip', 'zombie', 'zombies', 'zone',\n",
       "       'zones', 'zuma', 'prot_word_count'],\n",
       "      dtype='object', length=13757)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44dd8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.drop(['Unnamed: 0', 'target'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ef29d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aa', 'ab', 'abandon', 'abandoned', 'abandoning', 'abandonment',\n",
       "       'abbott', 'abc', 'aberration', 'abetted',\n",
       "       ...\n",
       "       'zionism', 'zionist', 'zionists', 'zip', 'zombie', 'zombies', 'zone',\n",
       "       'zones', 'zuma', 'prot_word_count'],\n",
       "      dtype='object', length=13755)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cd8cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_attr = ['gay', 'black', 'islam', 'christian', 'white', 'muslim', 'homosexual', 'chinese', 'bisexual', 'jew', 'catholic', 'transgender', 'latino', 'hindu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c290c9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46200\n",
      "19800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample, y, test_size = 0.3, stratify = y)\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e449598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aa', 'ab', 'abandon', 'abandoned', 'abandoning', 'abandonment',\n",
       "       'abbott', 'abc', 'aberration', 'abetted',\n",
       "       ...\n",
       "       'zionism', 'zionist', 'zionists', 'zip', 'zombie', 'zombies', 'zone',\n",
       "       'zones', 'zuma', 'prot_word_count'],\n",
       "      dtype='object', length=13755)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5afb842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "X_train_dropped = X_train.drop([\"prot_word_count\"], axis = 1)\n",
    "X_test_dropped = X_test.drop([\"prot_word_count\"], axis = 1)\n",
    "mim_aif = aif360_utils.FaXAIF(X_train_dropped, y_train, prot_attr, model_type = 'MIM')\n",
    "\n",
    "#predict the test data using the model\n",
    "# pred_prob = mim_aif.predict_proba(X_test_dropped)\n",
    "# print(pred_prob)\n",
    "pred = mim_aif.predict(X_test_dropped)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a30887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7876262626262627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df7ac364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7898484848484848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, max_iter = 400).fit(X_train_dropped, y_train)\n",
    "pred_baseline = clf.predict(X_test_dropped)\n",
    "print(accuracy_score(y_test, pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f548191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fair Model - % of non toxic comments in test having prot_attr and have been wrongly classified as toxic: 8.307908941563014\n",
      "Baseline Model - % of non toxic comments in test having prot_attr and have been wrongly classified as toxic: 9.72776343581319\n"
     ]
    }
   ],
   "source": [
    "X_test[\"y_test\"] = y_test\n",
    "X_test[\"y_preds\"] = pred\n",
    "X_test[\"y_preds_baseline\"] = pred_baseline\n",
    "\n",
    "not_toxic = X_test[X_test[\"y_test\"] == 0]\n",
    "not_toxic_having_prot_attr = not_toxic[not_toxic[\"prot_word_count\"] == 1]\n",
    "print(\"Fair Model - % of non toxic comments in test having prot_attr and have been wrongly classified as toxic: \"+str(len(not_toxic_having_prot_attr[not_toxic_having_prot_attr[\"y_preds\"] == 1])/len(not_toxic_having_prot_attr)*100))\n",
    "print(\"Baseline Model - % of non toxic comments in test having prot_attr and have been wrongly classified as toxic: \"+str(len(not_toxic_having_prot_attr[not_toxic_having_prot_attr[\"y_preds_baseline\"] == 1])/len(not_toxic_having_prot_attr)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b2a9cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding MDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd55cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "XZ = np.hstack((mim_aif.X, mim_aif.Z))\n",
    "mde_mim = FaX_methods.MDE_ind(XZ)\n",
    "inf_mim = mde_mim.influence(mim_aif.model, mim_aif.X)\n",
    "cols = list(X_train_compas.columns)[1:] +[list(X_train_compas.columns)[0]] \n",
    "res = {cols[i]: inf_mim[i] for i in range(len(inf_mim))}\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a274c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in prot_attr:\n",
    "    print(i)\n",
    "    print(res[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a43725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
